{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ad91a2-b545-47cf-9342-4d3fd3345b50",
   "metadata": {},
   "source": [
    "Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac3a0b2-3e3a-4de8-bfe7-32403213a6f3",
   "metadata": {},
   "source": [
    "Bayes' theorem, named after the Reverend Thomas Bayes, is a fundamental theorem in probability theory that describes the probability of an event based on prior knowledge of conditions that might be related to the event. Mathematically, it is represented as:\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)⋅P(A)\n",
    "\n",
    " \n",
    "\n",
    "Where:\n",
    "P(A∣B) is the posterior probability of event A given B.\n",
    "P(B∣A) is the likelihood of event B given A.\n",
    "P(A) and \n",
    "P(B) are the prior probabilities of events A and B, respectively.\n",
    "Bayes' theorem is commonly used in various fields such as statistics, machine learning, and Bayesian inference to update probabilities as new evidence becomes available. It provides a systematic way to incorporate new information into existing beliefs or hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c5af5-3374-4b3b-920e-4b8a71f20b96",
   "metadata": {},
   "source": [
    "Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f9aaa-ae26-4fef-a9af-c83d4e6cd534",
   "metadata": {},
   "source": [
    "Bayes' theorem is a fundamental principle in probability theory that describes the probability of an event based on prior knowledge or conditions that might be related to the event. It is represented by the following formula:\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)×P(A)\n",
    "Where:\n",
    "P(A∣B) is the posterior probability of event A given that event B has occurred.\n",
    "P(B∣A) is the likelihood or probability of event B occurring given that event A has occurred.\n",
    "P(A) is the prior probability of event A.\n",
    "P(B) is the prior probability of event B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71c7c4-02a6-4521-a810-91a395803721",
   "metadata": {},
   "source": [
    "Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965bf4d-af63-40c8-8e7d-b9c9c7b4cb87",
   "metadata": {},
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory that allows for the updating of probabilities based on new evidence. In practice, Bayes' theorem is utilized in various fields such as statistics, machine learning, medical diagnosis, spam filtering, and decision-making processes.\n",
    "\n",
    "One common application of Bayes' theorem is in medical diagnosis. Given a patient's symptoms and the prevalence of a particular disease within a population, Bayes' theorem can be used to calculate the probability that the patient has the disease. By incorporating additional diagnostic tests or symptoms, the probability can be updated accordingly, aiding in more accurate diagnoses.\n",
    "\n",
    "In machine learning, Bayes' theorem is used in Bayesian inference to update the probability of a hypothesis based on observed evidence. This approach is particularly useful in cases where data is limited or noisy, allowing for more robust model training and prediction.\n",
    "\n",
    "In spam filtering, Bayes' theorem is employed to classify emails as either spam or non-spam based on the presence of certain keywords or features. By calculating the probability that an email is spam given its content, spam filters can effectively identify and filter out unwanted messages.\n",
    "\n",
    "Furthermore, Bayes' theorem is applied in decision-making processes, such as in financial forecasting or risk assessment. By incorporating prior knowledge and updating probabilities based on new information, decision-makers can make more informed and rational choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a876e5bc-a23c-44ba-9652-e570c00cbea2",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb420b3-e3b3-44d3-bec2-1aada4b96094",
   "metadata": {},
   "source": [
    "Bayes' theorem is a fundamental principle in probability theory that relates conditional probabilities. It establishes a relationship between the probability of an event occurring given prior knowledge or evidence (posterior probability), the probability of the evidence given the occurrence of the event (likelihood), and the probability of the event occurring in general (prior probability). Mathematically, Bayes' theorem can be expressed as:\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)×P(A)\n",
    "Where:\n",
    "P(A∣B) is the conditional probability of event A given event B has occurred (posterior probability).\n",
    "P(B∣A) is the conditional probability of event B given event A has occurred (likelihood).\n",
    "P(A) and \n",
    "P(B) are the probabilities of events A and B occurring independently (prior probabilities).\n",
    "Bayes' theorem enables the updating of beliefs or probabilities based on new evidence, making it a powerful tool in fields such as statistics, machine learning, and artificial intelligence. It provides a systematic way to incorporate new information into existing knowledge, making predictions or decisions based on available evidence. The relationship between Bayes' theorem and conditional probability lies in its formulation, where it explicitly defines how the probability of an event given certain conditions can be calculated using conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a3bca-14d6-4e62-af57-4c02fbd2a261",
   "metadata": {},
   "source": [
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a7baf3-638e-4f1f-a8c2-cfaf42e4761a",
   "metadata": {},
   "source": [
    "When selecting a type of Naive Bayes classifier for a particular problem, several factors should be considered to make an informed decision:\n",
    "\n",
    "Nature of the Data:\n",
    "\n",
    "Gaussian Naive Bayes: Suitable for continuous data that follows a normal distribution.\n",
    "Multinomial Naive Bayes: Appropriate for discrete features, typically used in text classification with word counts or frequencies.\n",
    "Bernoulli Naive Bayes: Ideal for binary feature vectors, where features represent binary occurrences.\n",
    "Feature Independence Assumption:\n",
    "\n",
    "Gaussian Naive Bayes: Assumes features are continuous and follow a Gaussian distribution, while still considering independence among features.\n",
    "Multinomial Naive Bayes: Assumes features are discrete and follow a multinomial distribution, commonly used in text classification tasks.\n",
    "Bernoulli Naive Bayes: Assumes features are binary and follows a Bernoulli distribution, useful for binary feature vectors.\n",
    "Size of the Dataset:\n",
    "\n",
    "Gaussian Naive Bayes: Suitable for small to moderately sized datasets due to its simplicity and efficiency.\n",
    "Multinomial Naive Bayes: Often used for large datasets with high-dimensional feature spaces, commonly seen in text classification.\n",
    "Bernoulli Naive Bayes: Appropriate for binary feature vectors, particularly useful for datasets with a large number of features.\n",
    "Presence of Missing Values:\n",
    "\n",
    "Gaussian Naive Bayes: Can handle missing values by estimating the mean and variance from the available data.\n",
    "Multinomial Naive Bayes: Requires handling missing values before fitting the model, as it operates on discrete feature counts.\n",
    "Bernoulli Naive Bayes: Handles missing values by considering the absence of a feature as a separate category.\n",
    "Model Performance and Cross-Validation:\n",
    "\n",
    "Cross-validation: Employing techniques like k-fold cross-validation can help assess the performance of different Naive Bayes models on the given dataset.\n",
    "Model Evaluation Metrics: Consideration of appropriate evaluation metrics such as accuracy, precision, recall, and F1-score can guide the selection process.\n",
    "Domain Knowledge and Problem Context:\n",
    "\n",
    "Understanding the specific characteristics of the problem domain can provide insights into which Naive Bayes variant might be more suitable.\n",
    "Considering whether the independence assumption holds for the features in the dataset is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4212584-ba85-4b9a-a828-58e9512148fe",
   "metadata": {},
   "source": [
    "Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "A 3 3 4 4 3 3 3\n",
    "B 2 2 1 2 2 2 3\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740a36b-095c-4d53-8bab-23555f7fc038",
   "metadata": {},
   "source": [
    "To predict the class of a new instance using Naive Bayes classification, we utilize Bayes' theorem along with the assumption of conditional independence between features given the class. Given the provided dataset and equal prior probabilities for each class, we can calculate the conditional probabilities for each class based on the observed frequencies of feature values\n",
    "\n",
    "P(A) and P(B) as the prior probabilities for classes A and B respectively (assuming both are equal, hence P(A) = P(B) = 0.5).\n",
    "P(X1 = 3 | A) and P(X2 = 4 | A) as the conditional probabilities of feature values X1 = 3 and X2 = 4 given class A.\n",
    "P(X1 = 3 | B) and P(X2 = 4 | B) as the conditional probabilities of feature values X1 = 3 and X2 = 4 given class B.\n",
    "Using the provided frequencies, we can calculate these conditional probabilities as follows:\n",
    "\n",
    "For class A:\n",
    "\n",
    "P(X1 = 3 | A) = 4/13\n",
    "P(X2 = 4 | A) = 3/13\n",
    "For class B:\n",
    "\n",
    "P(X1 = 3 | B) = 1/9\n",
    "P(X2 = 4 | B) = 3/9\n",
    "Now, we apply Bayes' theorem to calculate the posterior probabilities for each class given the observed features:\n",
    "\n",
    "For class A:\n",
    "P(A | X1 = 3, X2 = 4) ∝ P(X1 = 3 | A) * P(X2 = 4 | A) * P(A)\n",
    "= (4/13) * (3/13) * (0.5)\n",
    "\n",
    "For class B:\n",
    "P(B | X1 = 3, X2 = 4) ∝ P(X1 = 3 | B) * P(X2 = 4 | B) * P(B)\n",
    "= (1/9) * (3/9) * (0.5)\n",
    "\n",
    "Now, we normalize these probabilities to sum up to 1:\n",
    "\n",
    "For class A:\n",
    "P(A | X1 = 3, X2 = 4) = (4/13) * (3/13) * (0.5) / ( (4/13) * (3/13) * (0.5) + (1/9) * (3/9) * (0.5) )\n",
    "\n",
    "For class B:\n",
    "P(B | X1 = 3, X2 = 4) = (1/9) * (3/9) * (0.5) / ( (4/13) * (3/13) * (0.5) + (1/9) * (3/9) * (0.5) )\n",
    "\n",
    "After calculating, we compare these probabilities. Whichever class has the higher posterior probability will be the predicted class for the new instance.\n",
    "\n",
    "Calculating the probabilities:\n",
    "\n",
    "For class A: P(A | X1 = 3, X2 = 4) ≈ 0.645\n",
    "For class B: P(B | X1 = 3, X2 = 4) ≈ 0.355\n",
    "Hence, Naive Bayes predicts the new instance to belong to class A since it has the higher posterior probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
